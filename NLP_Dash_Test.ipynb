{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\gensim\\utils.py:860: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Imports and setup for Jupyter\n",
    "\n",
    "import dash\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython import display\n",
    "import re\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import corpus\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import wordnet\n",
    "from nltk import punkt\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##function should allow to run in jupyter-- use show_app(#nameofapp) \n",
    "\n",
    "def show_app(app,  # type: dash.Dash\n",
    "             port=9999,\n",
    "             width=700,\n",
    "             height=900,\n",
    "             offline=True,\n",
    "             style=True,\n",
    "             **dash_flask_kwargs):\n",
    "    \"\"\"\n",
    "    Run the application inside a Jupyter notebook and show an iframe with it\n",
    "    :param app:\n",
    "    :param port:\n",
    "    :param width:\n",
    "    :param height:\n",
    "    :param offline:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    url = 'http://localhost:%d' % port\n",
    "    iframe = '<iframe src=\"{url}\" width={width} height={height}></iframe>'.format(url=url,\n",
    "                                                                                  width=width,\n",
    "                                                                                  height=height)\n",
    "    display.display_html(iframe, raw=True)\n",
    "    if offline:\n",
    "        app.css.config.serve_locally = True\n",
    "        app.scripts.config.serve_locally = True\n",
    "    if style:\n",
    "        external_css = [\"https://fonts.googleapis.com/css?family=Raleway:400,300,600\",\n",
    "                        \"https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css\",\n",
    "                        \"http://getbootstrap.com/dist/css/bootstrap.min.css\", ]\n",
    "\n",
    "        for css in external_css:\n",
    "            app.css.append_css({\"external_url\": css})\n",
    "\n",
    "        external_js = [\"https://code.jquery.com/jquery-3.2.1.min.js\",\n",
    "                       \"https://cdn.rawgit.com/plotly/dash-app-stylesheets/a3401de132a6d0b652ba11548736b1d1e80aa10d/dash-goldman-sachs-report-js.js\",\n",
    "                       \"http://getbootstrap.com/dist/js/bootstrap.min.js\"]\n",
    "\n",
    "        for js in external_js:\n",
    "            app.scripts.append_script({\"external_url\": js})\n",
    "\n",
    "    return app.run_server(debug=False,  # needs to be false in Jupyter\n",
    "                          port=port,\n",
    "                          **dash_flask_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Dash Notes\n",
    "\n",
    "anything that starts with html. ... is generating pure HTML, anything that comes from the dcc. library is using things like Javascript, CSS and HTML\n",
    "\n",
    "can customize the CSS styles of the app---app.css.append_css()\n",
    "\n",
    "have to stop the kernal and restart each time you'd like to 'retest' the app--- guessing that it stays running so that you can make interactive changes to it.\n",
    "\n",
    "style property is best to supply a dictionary to (colors example)\n",
    "\n",
    "dcc.Graph renders interactive data viz using JavaScript graphing library.\n",
    "\n",
    "can write text in markdown... set something text equal to a variable and then use dcc.Markdown function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call backs (interactivity) notes\n",
    "\n",
    "need to import dash.depencies (input output)\n",
    "\n",
    "describe inputs and outputs through a app.callback decorator\n",
    "\n",
    "the inputs and outputs of the application are simply the properties of a particular component (so for example a dcc.Input)\n",
    "\n",
    "don't set a value for the children property of the my-div component in the layout... when Dash starts it automatically calls all of the callbacks with the inital values of the input components in order to population the initial start\n",
    "\n",
    "It's reactive programming\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning:\n",
      "\n",
      "No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Importing the final report\n",
    "\n",
    "##can I call something from another jupyter notebook? Seems like we might need to either have a massive setup beforehand or\n",
    "##save the other file as a .py script and then import like that\n",
    "\n",
    "report=pd.read_csv('connect_report.csv')\n",
    "\n",
    "report['Created Date']=pd.to_datetime(report['Created Date'])\n",
    "report=report[pd.notnull(report['Created Date'])]\n",
    "report=report[pd.notnull(report['Body'])]\n",
    "\n",
    "#Removing HTML tags and codes\n",
    "report['Body'] = report['Body'].apply(lambda x:BeautifulSoup(x))\n",
    "report['Body'] = report['Body'].apply(lambda x:x.get_text())\n",
    "\n",
    "#Removing tags/mentions\n",
    "report['Body'] = report['Body'].apply(lambda x:re.sub('{@[\\w\\d]*}', '',x))\n",
    "\n",
    "#Removing urls\n",
    "report['Body'] = report['Body'].apply(lambda x:re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x,flags=re.MULTILINE))\n",
    "\n",
    "#Removing weird unicode characters\n",
    "report['Body'] = report['Body'].apply(lambda x: x.encode('ascii', 'ignore'))\n",
    "\n",
    "\n",
    "final_report=pd.DataFrame\n",
    "\n",
    "#Use this function to create the final report\n",
    "# Directions:\n",
    "# 1. Set Group to a list of group names, a single name of a group, or All to use all groups\n",
    "# 2. Set Type to a specific Feed Item Type or All\n",
    "# 3. Set Start Date or leave blank(defaults to: )\n",
    "# 4. Set End Date or leave blank(defaults to: )\n",
    "\n",
    "# def report_generator(Group,Type,Date_start=pd.to_datetime(\"2017-01-01\"),Date_finish=pd.to_datetime('today')):\n",
    "#     if type(Group) == list:\n",
    "#         final_report=report.loc[report['Name'].isin(Group)] \n",
    "#     elif (Group=='All'):\n",
    "#         final_report=report\n",
    "#     else:\n",
    "#         final_report=report.loc[report['Name']==Group]\n",
    "                                \n",
    "#     if (Type=='All'):\n",
    "#         final_report=final_report\n",
    "#     else:\n",
    "#         final_report=final_report.loc[final_report['Feed Item Type']==Type]\n",
    "    \n",
    "#     if(Date_start==\"2017-01-01\" and Date_finish== pd.to_datetime('today')):\n",
    "#         final_report=final_report\n",
    "#     else:\n",
    "#         Date_start=pd.to_datetime(Date_start, utc=False)\n",
    "#         Date_finish=pd.to_datetime(Date_finish, utc=False)\n",
    "#         #alt approach-- not working but not sure why-- it worked it worked it worked! had to change the data type to all be annoying ass datatime64... down the line that might be a pain but for now it's dope\n",
    "#         final_report=final_report[(final_report['Created Date'] > Date_start) & (final_report['Created Date']< Date_finish)]   \n",
    "#     return(final_report)\n",
    "\n",
    "# final_report=report_generator(['CPC+ All','NLT Internal Users'],'All',\"2017-02-01\",\"2017-03-01\")\n",
    "\n",
    "\n",
    "def report_generator(Group,Type):\n",
    "    if type(Group) == list:\n",
    "        final_report=report.loc[report['Name'].isin(Group)] \n",
    "    elif (Group=='All'):\n",
    "        final_report=report\n",
    "    else:\n",
    "        final_report=report.loc[report['Name']==Group]\n",
    "                                \n",
    "    if (Type=='All'):\n",
    "        final_report=final_report\n",
    "    else:\n",
    "        final_report=final_report.loc[final_report['Feed Item Type']==Type]     \n",
    "    return(final_report)\n",
    "\n",
    "final_report=report_generator(['CPC+ All','NLT Internal Users'],'All')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stop.append(\"cpc+\")\n",
    "stop.append(\"hi\")\n",
    "stop.append(\"hello\")\n",
    "stop.append(\"also\")\n",
    "stop.append(\"anyone\")\n",
    "stop.append(\"et\")\n",
    "stop.append(\"please\")\n",
    "\n",
    "wnl = WordNetLemmatizer() \n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    " \n",
    "    tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text.lower())]\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token_by_sent in tokens_:\n",
    "        tokens += token_by_sent\n",
    " \n",
    "    tokens = list(filter(lambda t: t not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\",u\"'ve\",u\"'re\",u\"'d\",u\"'ll\",u\"'m\", u\"u\", \n",
    "                                             u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "     \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wnl.lemmatize(token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    " \n",
    "    filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "    \n",
    "    bigrm = nltk.bigrams(filtered_tokens)\n",
    "    bi_tokens = []\n",
    "    for a, b in bigrm:\n",
    "        bi_tok = ' '.join((a, b))\n",
    "        bi_tokens.append(bi_tok)\n",
    "    \n",
    "    filtered_tokens.extend(bi_tokens)\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def single_day_tracker(report):\n",
    "    #Create the top 10 list\n",
    "    unlisted_report=report['tokens'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "    word_vectorizer_2=CountVectorizer(ngram_range=(1,1), analyzer='word')\n",
    "    word_vectorizer_fit_2=word_vectorizer_2.fit_transform(unlisted_report)\n",
    "\n",
    "    frequencies=sum(word_vectorizer_fit_2).toarray()[0]\n",
    "\n",
    "    bigrams_counter=pd.DataFrame(frequencies,index=word_vectorizer_2.get_feature_names(),columns=['frequency'])\n",
    "    f = bigrams_counter[bigrams_counter['frequency']>10]\n",
    "    \n",
    "   \n",
    "    f=f.sort_values(by=['frequency'],ascending=False)\n",
    "    top_words=f.head(15)\n",
    "    \n",
    "    top_words_listed=list(top_words.index)\n",
    "\n",
    "\n",
    "    #Okay now figure out what the frequency was each day\n",
    "    unique_dates=set(report['Created Date'])\n",
    "    list_dates=sorted(list(unique_dates))\n",
    "    list_df_name=list(xrange(0,len(list_dates)))\n",
    "    list_df=[]\n",
    "    new_df=pd.DataFrame\n",
    "    for i in xrange(0,len(list_dates)):\n",
    "        date_report=report.loc[report['Created Date']==list_dates[i]]\n",
    "        unlisted_report_date=unlisted_report=date_report['tokens'].apply(lambda x: ', '.join(x))\n",
    "        word_vectorizer_fit_date=word_vectorizer_2.fit_transform(unlisted_report_date)\n",
    "        frequencies_date=sum(word_vectorizer_fit_date).toarray()[0]\n",
    "        bigrams_counter_date=pd.DataFrame(frequencies_date,index=word_vectorizer_2.get_feature_names(),columns=['frequency'])\n",
    "        bigrams_date_listed=list(bigrams_counter_date.index)\n",
    "        finder=(bigrams_counter_date.index.isin(top_words.index))\n",
    "        slimmed=(bigrams_counter_date[finder])\n",
    "        slimmed['word']=slimmed.index\n",
    "        slimmed['date']=list_dates[i]\n",
    "        slimmed['date']=pd.to_datetime(slimmed['date'])\n",
    "        list_df_name[i]=slimmed\n",
    "        list_df.append(list_df_name[i])\n",
    "    new_df=pd.concat(list_df)\n",
    "    new_df.index=xrange(0,len(new_df['word']))\n",
    "    return(new_df)\n",
    "\n",
    "##MOVED INTO REACTIVE SECTION BELOW\n",
    "            \n",
    "# top_words=single_day_tracker(final_report)\n",
    "\n",
    "\n",
    "    \n",
    "# #Bigrams/collocation scoring\n",
    "# bigram_measures=nltk.collocations.BigramAssocMeasures()\n",
    "# finder_2=BigramCollocationFinder.from_documents(final_report['tokens'])\n",
    "# score_collocation_2=finder_2.nbest(bigram_measures.pmi,5)\n",
    "\n",
    "# scored = finder_2.score_ngrams(bigram_measures.pmi)\n",
    "# sorted_scored=sorted(bigram for bigram, score in scored)\n",
    "\n",
    "\n",
    "\n",
    "#Single word finder\n",
    "def single_word_finder(x,y,mode='single'):\n",
    "    scores_e=list(enumerate(scored))\n",
    "    index_list=[]\n",
    "    final_list=[]\n",
    "    if(mode=='double'):\n",
    "        for i, v in enumerate(scored):\n",
    "            if v[0] == (x,y):\n",
    "                return scores_e[i]\n",
    "            elif v[0] == (y,x):\n",
    "                return scores_e[i]\n",
    "    elif(mode=='single'):\n",
    "        for i, v in enumerate(scored):\n",
    "            if(scores_e[i][1][0][0]) == x or scores_e[i][1][0][1] == x or scores_e[i][1][0][0]==y or scores_e[i][1][0][1]==y:\n",
    "                index_list.append(i)\n",
    "        for j in index_list:\n",
    "            final_list.append(scores_e[j])\n",
    "        return final_list\n",
    "\n",
    "def graphing_single_word_finder(word1,word2='',mode='single'):\n",
    "    graphing_data=single_word_finder(word1,'','single')\n",
    "    graphing_data=pd.Series(graphing_data)\n",
    "    score_list=[]\n",
    "    non_word1_list=[]\n",
    "    for i,v in graphing_data.iteritems():\n",
    "        if(graphing_data[i][1][0][0]!=word1):\n",
    "            non_word1_list.append(graphing_data[i][1][0][0])\n",
    "        elif(graphing_data[i][1][0][1])!=word1:\n",
    "            non_word1_list.append(graphing_data[i][1][0][1])\n",
    "        score_list.append(graphing_data[i][1][1])\n",
    "    final_graphing=pd.concat([pd.Series(non_word1_list),pd.Series(score_list)],axis=1)\n",
    "    final_graphing.columns=['Word','PMI_Score']\n",
    "    sns.mpl.rc(\"figure\", figsize=(9,4))\n",
    "    word_graph=sns.stripplot(x=\"Word\", y=\"PMI_Score\", data=final_graphing, size = 8)\n",
    "    loc, labels = plt.xticks()\n",
    "    word_graph.set_xticklabels(labels, rotation=90)\n",
    "    return(final_graphing)\n",
    "\n",
    "def graphing_labels(word1,word2='',mode='single'):\n",
    "    graphing_data=single_word_finder(word1,'','single')\n",
    "    graphing_data=pd.Series(graphing_data)\n",
    "    score_list=[]\n",
    "    non_word1_list=[]\n",
    "    for i,v in graphing_data.iteritems():\n",
    "        if(graphing_data[i][1][0][0]!=word1):\n",
    "            non_word1_list.append(graphing_data[i][1][0][0])\n",
    "        elif(graphing_data[i][1][0][1])!=word1:\n",
    "            non_word1_list.append(graphing_data[i][1][0][1])\n",
    "        score_list.append(graphing_data[i][1][1])\n",
    "    final_graphing=pd.concat([pd.Series(non_word1_list),pd.Series(score_list)],axis=1)\n",
    "    final_graphing.columns=['Word','PMI_Score']\n",
    "    return(final_graphing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Setting up table generator example\n",
    "def generate_table(dataframe, maxrows=10):\n",
    "    return html.Table(\n",
    "        # Header\n",
    "        [html.Tr([html.Th(col) for col in dataframe.columns])] +\n",
    "\n",
    "        # Body\n",
    "        [html.Tr([\n",
    "            html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "        ]) for i in range(min(len(dataframe), maxrows))]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Text Example\n",
    "\n",
    "example_text='''\n",
    "## This is where we would write an intro.\n",
    "\n",
    "Maybe something about how to use the dashboard, or how to get it to run.\n",
    "Explanation of scores, things like that.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Setting up a test environment\n",
    "\n",
    "\n",
    "\n",
    "app=dash.Dash()\n",
    "\n",
    "\n",
    "\n",
    "colors={\n",
    "    'background': '#111111',\n",
    "    'text': '#7FDBFF'\n",
    "}\n",
    "\n",
    "app.layout=html.Div(children=\n",
    "    [html.H1(children='This is the title pane',\n",
    "             style={\n",
    "                 'textAlign': 'center', 'color':colors['text']}   ),\n",
    "    dcc.Markdown(children=example_text), \n",
    "    html.H2(children='Please use these inputs to customize your report',\n",
    "            style={\n",
    "                 'textAlign': 'center', 'color':colors['text']} ),\n",
    "    dcc.DatePickerRange(\n",
    "        id='date_selector',\n",
    "        min_date_allowed=dt(2017,1,1),\n",
    "        end_date=dt.today()),\n",
    "    html.Table(id='table'), \n",
    "#Need to work on spacing here and then figure out how to use them as arguments for all of the functions  \n",
    "#     html.Label('First word finder'),\n",
    "#     dcc.Input(\n",
    "#         id='first_word_finder',\n",
    "#         value='',\n",
    "#         type='text'),\n",
    "#     html.Label('Second word finder'),\n",
    "#     dcc.Input(value='If you would like to find the intersection of the first word and another please enter here',type='text'),\n",
    "#     ##Try to build the graph here \n",
    "#     dcc.Graph(\n",
    "#         id='test_graph', \n",
    "        \n",
    "#     ),\n",
    "     \n",
    "    \n",
    "     \n",
    "     html.Div(id='output-container-date-selector',style={'display': 'none'}),\n",
    "     html.Div(id='output-container-top-words',style={'display':'none'}),\n",
    "     html.Div(id=\"output-container-scored\",style={'display':'none'})\n",
    "     \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Need to move things that are setup/dependent on reactive inputs here into here\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output-container-date-selector','children'),\n",
    "[dash.dependencies.Input('date_selector','start_date'),\n",
    " dash.dependencies.Input('date_selector','end_date')]\n",
    ")\n",
    "##going to need to do this in a bunch of different functions?\n",
    "def update_report(final_report,input1,input2):\n",
    "    if(input1==\"2017-01-01\" and input2== pd.to_datetime('today')):\n",
    "        final_report_reactive=final_report\n",
    "    else:\n",
    "        start=pd.to_datetime(input1, utc=False)\n",
    "        end=pd.to_datetime(input2, utc=False)\n",
    "        final_report_reactive=final_report[(final_report['Created Date'] > start) & (final_report['Created Date']< end)]   \n",
    "    return(final_report_reactive.to_json())\n",
    "\n",
    "#Testing on the table\n",
    "@app.callback(dash.dependencies.Output('table','children'),\n",
    "             [dash.dependencies.Input('output-container-date-selector','children')]\n",
    "             )\n",
    "def update_table(input1):\n",
    "    final_report_updated=pd.read_json(input1)\n",
    "    reactive_table=create_table(final_report_updated)\n",
    "    return(reactive_table)\n",
    "\n",
    "# @app.callback(dash.dependencies.Output('output-container-top-words','children'),\n",
    "#              dash.dependencies.Input('output-container-date-selector','children')\n",
    "#              )\n",
    "# def update_top_words(input):\n",
    "#     top_words_reactive=single_day_tracker(input)\n",
    "#     return(top_words_reactive.to_json())\n",
    "\n",
    "# @app.callback(dash.dependencies.Output('output-container-scored','children'),\n",
    "#             [dash.dependencies.Input('output-container-top-words','children'),\n",
    "#               dash.dependencies.Input('output-container-date-selector','children')]\n",
    "#              )\n",
    "# def update_scores(input1): \n",
    "# #Bigrams/collocation scoring\n",
    "#     bigram_measures=nltk.collocations.BigramAssocMeasures()\n",
    "#     finder_2=BigramCollocationFinder.from_documents(input2['tokens'])\n",
    "#     score_collocation_2=finder_2.nbest(bigram_measures.pmi,5)\n",
    "\n",
    "#     scored = finder_2.score_ngrams(bigram_measures.pmi)\n",
    "#     sorted_scored_reactive=sorted(bigram for bigram, score in scored)\n",
    "#     return(sorted_scored_reactive)\n",
    "\n",
    "\n",
    "# @app.callback(\n",
    "#     dash.dependencies.Output('test_graph','figure'),\n",
    "#     [dash.dependencies.Input('first_word_finder','value'),\n",
    "#     dash.dependencies.Input('output-container-scored')]\n",
    "# )\n",
    "\n",
    "# def update_graph(input1,input2):\n",
    "\n",
    "#     pmi_score=graphing_single_word_finder(input1,'',mode='single')\n",
    "#     g_labels=graphing_labels(input2,'',mode='single')\n",
    "#     g_labels=g_labels['Word'].tolist()\n",
    "#     return{\n",
    "#             'data': [\n",
    "#                 go.Scatter(\n",
    "#                 x=pmi_score['Word'],\n",
    "#                 y=pmi_score['PMI_Score'],\n",
    "#                 mode='markers',\n",
    "#                     marker=dict(\n",
    "#                     color=range(0,len(pmi_score)),\n",
    "#                     cmin=0,\n",
    "#                     cmax=28,\n",
    "#                     colorscale='Jet'\n",
    "#                 )\n",
    "#                 )\n",
    "#                 ],\n",
    "#             'layout':go.Layout(\n",
    "#                 xaxis={'title':'PMI for Searched words','ticktext':g_labels}\n",
    "#                 )\n",
    "#    }\n",
    "\n",
    "\n",
    "show_app(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [07/Dec/2017 10:10:51] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Dec/2017 10:10:53] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Dec/2017 10:10:53] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Dec/2017 10:10:53] \"POST /_dash-update-component HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Dec/2017 10:10:55] \"GET /favicon.ico HTTP/1.1\" 200 -\n",
      "[2017-12-07 10:10:56,141] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\flask\\app.py\", line 1982, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\flask\\app.py\", line 1614, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\flask\\app.py\", line 1517, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\flask\\app.py\", line 1612, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\flask\\app.py\", line 1598, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\dash\\dash.py\", line 541, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\dash\\dash.py\", line 509, in add_context\n",
      "    cls=plotly.utils.PlotlyJSONEncoder),\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\json\\__init__.py\", line 251, in dumps\n",
      "    sort_keys=sort_keys, **kw).encode(obj)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\plotly\\utils.py\", line 136, in encode\n",
      "    encoded_o = super(PlotlyJSONEncoder, self).encode(o)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\json\\encoder.py\", line 207, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\json\\encoder.py\", line 270, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\site-packages\\plotly\\utils.py\", line 204, in default\n",
      "    return _json.JSONEncoder.default(self, obj)\n",
      "  File \"C:\\Users\\583185\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py27\\lib\\json\\encoder.py\", line 184, in default\n",
      "    raise TypeError(repr(o) + \" is not JSON serializable\")\n",
      "TypeError:                                                    Body  Comment Count  \\\n",
      "10                        We are using eClinicalworks\\n              2   \n",
      "100   can we also have a grp for CPC providers in NJ?\\n              0   \n",
      "1000  Can we user our care management fees to provid...              4   \n",
      "1001  Hello! We are just minutes away from kicking o...              0   \n",
      "1002  Could a Nurse Practitioner with additional beh...              1   \n",
      "1003  On the list of behavioral health integration o...              1   \n",
      "1004  For quarterly Care Delivery reporting, when yo...             15   \n",
      "1005  Hi everyone!\\n\\nThank you for joining us for s...             13   \n",
      "1006  OUTAGE ALERT: Due to maintenance, the CPC+ Pra...              0   \n",
      "1007  You can learn about best practices for risk st...              0   \n",
      "1008  Hi everyone!Thank you all for attending sessio...              1   \n",
      "1009  For the attribution calculations that will tak...              8   \n",
      "101   Plan ahead! Due to maintenance, the CPC+ Pract...              0   \n",
      "1010  Can anyone tell me when I should update the pr...              3   \n",
      "1011  Our team at Practice Plus, Arkansas Health Gro...              6   \n",
      "1012  CARE GIVER HOTLINE GOES ACTIVE. Meetinghouse F...             10   \n",
      "1013  Has the recording of Session 2 (July 11) of St...              1   \n",
      "1014  Has the link been posted to access the webinar...              4   \n",
      "1015  We are looking forward to a great discussion o...              0   \n",
      "1016  Hello! We are minutes away from the live sessi...              3   \n",
      "1017  @CPC+All Does the Cologuard count in the color...             12   \n",
      "1018               I didn't get it in my inbox either\\n              1   \n",
      "102   Now that you?ve joined the CPC+ Help and Enhan...              0   \n",
      "1021  Any suggestion on how everyone is using their ...              2   \n",
      "1022  Hello-\\nI was unable to connect to the session...              1   \n",
      "1023  Looking forward to seeing Quarter 2 Feedback R...              3   \n",
      "1024  Hi All! We hope you can all join us for our fi...              0   \n",
      "1025  Thank you for hosting AG 015 Session 1 today. ...              2   \n",
      "1026  Wanting to try to startimplementing phone call...              7   \n",
      "1027  Good morning,\\n\\nI'm just wondering when the B...              2   \n",
      "...                                                 ...            ...   \n",
      "972   Our accounting manager is asking about trackin...             31   \n",
      "973   Looking forward to seeing Quarter 2 Feedback R...             21   \n",
      "974   You can learn about best practices for risk st...              0   \n",
      "975   We are experiencing a problem with our EHR IT....              2   \n",
      "976   Check out this week?s edition of On the Plus S...              0   \n",
      "977   How is everyone doing with your risk stratific...              0   \n",
      "978   How is everyone doing with your risk stratific...              6   \n",
      "979   Can anyone share with me a copy of the AAFP or...              2   \n",
      "98    We are happy to announce that health IT vendor...              0   \n",
      "980   Are you a practice that uses IBM Watson Health...              0   \n",
      "981   Can we include eligible encounters provided by...              1   \n",
      "982   Hi everyone!\\n\\nJust as a reminder, our Behavi...              3   \n",
      "983   Hi everyone!\\n\\nWe look forward to having you ...              0   \n",
      "984   during yesterday's session I thought I heard t...              2   \n",
      "985   Does participation in CPC+ exclude a practice ...              0   \n",
      "986   Are any practices changing the way they pay th...             14   \n",
      "987   My practice has two podiatrists physically loc...              3   \n",
      "988   We are1 houraway from kicking off the first se...              1   \n",
      "989   Any idea when we will get the Financial Foreca...             20   \n",
      "99    Can we please start a grp based on ACO affilia...              0   \n",
      "990   Hello! Provided below is the AG009 charter. We...              1   \n",
      "991   I registered for this action group's 1st sessi...              5   \n",
      "992   Do anyone have any clear, concise language tha...              3   \n",
      "993   I have attached the Residency Group audio from...              3   \n",
      "994   Hi everyone, registration for Action Group 009...              0   \n",
      "995   Hi everyone, registration for Action Group 016...              0   \n",
      "996   Hello All,\\nQuestion regarding hospital admit ...              4   \n",
      "997                                     BHI Worksheet\\n              1   \n",
      "998   Hi everyone,\\nWe're excited to kick off sessio...              0   \n",
      "999   Hello!The videorecording for Session 3 which o...              0   \n",
      "\n",
      "                            Created By: Company Name Created By: Full Name  \\\n",
      "10                                                MI       Karen Mcauliffe   \n",
      "100                                               NJ     Kavitha Thukkaram   \n",
      "1000                        Union Physician Services           Teresa Goss   \n",
      "1001                             Booz Allen Hamilton      Olufunke Adefemi   \n",
      "1002       North Central Arkansas Medical Associates        Kenderia Perry   \n",
      "1003       North Central Arkansas Medical Associates        Kenderia Perry   \n",
      "1004                      Integrated Health Partners       Krystal Schramm   \n",
      "1005                                 HealthTeamWorks         Jennifer Boaz   \n",
      "1006                             Booz Allen Hamilton          Walker Sands   \n",
      "1007                             Booz Allen Hamilton          Walker Sands   \n",
      "1008                                 HealthTeamWorks         Jennifer Boaz   \n",
      "1009                      Integrated Health Partners       Krystal Schramm   \n",
      "101                              Booz Allen Hamilton          Walker Sands   \n",
      "1010                       Matthews-Vu Medical Group        Sara Sanderson   \n",
      "1011             Practice Plus Arkansas Health Group           Charles Cox   \n",
      "1012                  Meetinghouse Family Physicians        Sloan Robinson   \n",
      "1013                                     Lake Health            Julia Heng   \n",
      "1014             Practice Plus Arkansas Health Group       Patricia Gibson   \n",
      "1015                                             BAH            Amy Gibson   \n",
      "1016                             Booz Allen Hamilton      Olufunke Adefemi   \n",
      "1017                                              AR        Candi Martinez   \n",
      "1018             Practice Plus Arkansas Health Group        Shelly Spollen   \n",
      "102                              Booz Allen Hamilton          Walker Sands   \n",
      "1021                                 St. John Clinic        Christine Sims   \n",
      "1022                                              OH         Sandy Johnson   \n",
      "1023                             Booz Allen Hamilton          Walker Sands   \n",
      "1024                                             HTW         Elisha Jewett   \n",
      "1025                          Hawaii Health Partners           Barry Major   \n",
      "1026                       Medical Associates of NWA       Erika Henderson   \n",
      "1027  Premier Medical Group of the Hudson Valley, PC          Lauren Roach   \n",
      "...                                              ...                   ...   \n",
      "972                  Glenwood Medical Associates, PC             Ivy Davis   \n",
      "973                              Booz Allen Hamilton          Walker Sands   \n",
      "974                              Booz Allen Hamilton          Walker Sands   \n",
      "975                                               OR       Julie Bumgarner   \n",
      "976                                              BAH           Sam Gottuso   \n",
      "977      National Learning Network - HealthTeamWorks       Jennifer Miller   \n",
      "978      National Learning Network - HealthTeamWorks       Jennifer Miller   \n",
      "979                                               MI       Deanna Rawlings   \n",
      "98                               Booz Allen Hamilton        Jonathan Perry   \n",
      "980                              Booz Allen Hamilton          Walker Sands   \n",
      "981                         Sky Lakes Medical Center       Christa Runnels   \n",
      "982                                  HealthTeamWorks         Jennifer Boaz   \n",
      "983                                  HealthTeamWorks         Jennifer Boaz   \n",
      "984      New Providence Internal Medicine Associates       Leonora Swanton   \n",
      "985                       Grants Pass Clinic, L.L.P.          Faith Tuttle   \n",
      "986                  Foresight Family Physicians, PC        Gregory Reicks   \n",
      "987                       Grants Pass Clinic, L.L.P.          Faith Tuttle   \n",
      "988                              Booz Allen Hamilton      Olufunke Adefemi   \n",
      "989                   Kalispell Regional Health Care   Jacqueline Dittmann   \n",
      "99                                                NJ     Kavitha Thukkaram   \n",
      "990                              Booz Allen Hamilton      Olufunke Adefemi   \n",
      "991             St. Luke's University Health Network        Jessica Rivera   \n",
      "992                       Saint Luke's Health System           Amanda Holt   \n",
      "993                     TMF Health Quality Institute        Francesca High   \n",
      "994                              Booz Allen Hamilton          Walker Sands   \n",
      "995                              Booz Allen Hamilton          Walker Sands   \n",
      "996                                               NJ       Carol Ann Logan   \n",
      "997                                  HealthTeamWorks         Jennifer Boaz   \n",
      "998                                  HealthTeamWorks         Jennifer Boaz   \n",
      "999                              Booz Allen Hamilton      Olufunke Adefemi   \n",
      "\n",
      "       Created Date Feed Item Type  Like Count  \\\n",
      "10    1485734400000      Text Post           0   \n",
      "100   1488412800000      Text Post           0   \n",
      "1000  1500508800000      Text Post           3   \n",
      "1001  1500508800000   Content Post           1   \n",
      "1002  1500508800000      Text Post           0   \n",
      "1003  1500508800000      Text Post           2   \n",
      "1004  1500508800000      Text Post           3   \n",
      "1005  1500595200000      Text Post           1   \n",
      "1006  1500595200000      Text Post           1   \n",
      "1007  1500595200000   Content Post           2   \n",
      "1008  1500595200000      Text Post           2   \n",
      "1009  1500595200000      Text Post           1   \n",
      "101   1488412800000      Text Post           0   \n",
      "1010  1500595200000      Text Post           0   \n",
      "1011  1500595200000      Text Post           3   \n",
      "1012  1500681600000      Text Post          10   \n",
      "1013  1500768000000      Text Post           0   \n",
      "1014  1500854400000      Text Post           0   \n",
      "1015  1500854400000   Content Post           1   \n",
      "1016  1500854400000   Content Post           0   \n",
      "1017  1500854400000      Text Post           4   \n",
      "1018  1500854400000      Text Post           0   \n",
      "102   1488412800000      Link Post           2   \n",
      "1021  1500854400000      Text Post           2   \n",
      "1022  1500854400000      Text Post           0   \n",
      "1023  1500854400000      Text Post           1   \n",
      "1024  1500854400000      Text Post           1   \n",
      "1025  1500854400000      Text Post           2   \n",
      "1026  1500940800000      Text Post           2   \n",
      "1027  1500940800000      Text Post           0   \n",
      "...             ...            ...         ...   \n",
      "972   1500336000000      Text Post          14   \n",
      "973   1500336000000      Text Post           6   \n",
      "974   1500336000000   Content Post           1   \n",
      "975   1500336000000      Text Post           1   \n",
      "976   1500422400000   Content Post           0   \n",
      "977   1500422400000   Content Post           1   \n",
      "978   1500422400000   Content Post           0   \n",
      "979   1500422400000      Text Post           0   \n",
      "98    1488412800000   Content Post           5   \n",
      "980   1500422400000      Text Post           1   \n",
      "981   1500422400000      Text Post           2   \n",
      "982   1500422400000      Text Post           4   \n",
      "983   1500422400000   Content Post           1   \n",
      "984   1500422400000      Text Post           1   \n",
      "985   1500422400000      Link Post           1   \n",
      "986   1500422400000      Text Post          11   \n",
      "987   1500422400000      Text Post           6   \n",
      "988   1500422400000   Content Post           1   \n",
      "989   1500422400000      Text Post          15   \n",
      "99    1488412800000      Text Post           1   \n",
      "990   1500508800000   Content Post           0   \n",
      "991   1500508800000      Text Post           0   \n",
      "992   1500508800000      Text Post           6   \n",
      "993   1500508800000   Content Post           0   \n",
      "994   1500508800000      Link Post           0   \n",
      "995   1500508800000      Link Post           0   \n",
      "996   1500508800000      Text Post           0   \n",
      "997   1500508800000   Content Post           2   \n",
      "998   1500508800000      Text Post           1   \n",
      "999   1500508800000   Content Post           1   \n",
      "\n",
      "                                          Name     Network: Name  \n",
      "10                                    CPC+ All  CPC Plus Connect  \n",
      "100                 CPC+ Help and Enhancements  CPC Plus Connect  \n",
      "1000                                  CPC+ All  CPC Plus Connect  \n",
      "1001   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1002   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1003   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1004                                  CPC+ All  CPC Plus Connect  \n",
      "1005               AG012: Optimizing Health IT  CPC Plus Connect  \n",
      "1006                                  CPC+ All  CPC Plus Connect  \n",
      "1007                                  CPC+ All  CPC Plus Connect  \n",
      "1008   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1009                                  CPC+ All  CPC Plus Connect  \n",
      "101                                   CPC+ All  CPC Plus Connect  \n",
      "1010                                  CPC+ All  CPC Plus Connect  \n",
      "1011   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1012                                  CPC+ All  CPC Plus Connect  \n",
      "1013                    AG011: Starting a PFAC  CPC Plus Connect  \n",
      "1014   AG015: Integrating Behavioral Health:PC  CPC Plus Connect  \n",
      "1015   AG015: Integrating Behavioral Health:PC  CPC Plus Connect  \n",
      "1016   AG015: Integrating Behavioral Health:PC  CPC Plus Connect  \n",
      "1017                                  CPC+ All  CPC Plus Connect  \n",
      "1018   AG015: Integrating Behavioral Health:PC  CPC Plus Connect  \n",
      "102                 CPC+ Help and Enhancements  CPC Plus Connect  \n",
      "1021                                  CPC+ All  CPC Plus Connect  \n",
      "1022   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "1023                                  CPC+ All  CPC Plus Connect  \n",
      "1024                    AG011: Starting a PFAC  CPC Plus Connect  \n",
      "1025   AG015: Integrating Behavioral Health:PC  CPC Plus Connect  \n",
      "1026                 Role Group: Care Managers  CPC Plus Connect  \n",
      "1027   AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "...                                        ...               ...  \n",
      "972                                   CPC+ All  CPC Plus Connect  \n",
      "973                                   CPC+ All  CPC Plus Connect  \n",
      "974                                   CPC+ All  CPC Plus Connect  \n",
      "975                                   CPC+ All  CPC Plus Connect  \n",
      "976                                   CPC+ All  CPC Plus Connect  \n",
      "977                 AG001: Risk Stratification  CPC Plus Connect  \n",
      "978                 AG002: Risk Stratification  CPC Plus Connect  \n",
      "979                 AG002: Risk Stratification  CPC Plus Connect  \n",
      "98                                    CPC+ All  CPC Plus Connect  \n",
      "980                                   CPC+ All  CPC Plus Connect  \n",
      "981                                   CPC+ All  CPC Plus Connect  \n",
      "982                                   CPC+ All  CPC Plus Connect  \n",
      "983                AG012: Optimizing Health IT  CPC Plus Connect  \n",
      "984             AG013: Self Management Support  CPC Plus Connect  \n",
      "985                                   CPC+ All  CPC Plus Connect  \n",
      "986                                   CPC+ All  CPC Plus Connect  \n",
      "987                                   CPC+ All  CPC Plus Connect  \n",
      "988                AG012: Optimizing Health IT  CPC Plus Connect  \n",
      "989                                   CPC+ All  CPC Plus Connect  \n",
      "99                  CPC+ Help and Enhancements  CPC Plus Connect  \n",
      "990   AG009:Beyond Traditional Visit with CPCP  CPC Plus Connect  \n",
      "991    AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "992                                   CPC+ All  CPC Plus Connect  \n",
      "993                Residency-Focused Practices  CPC Plus Connect  \n",
      "994                                   CPC+ All  CPC Plus Connect  \n",
      "995                                   CPC+ All  CPC Plus Connect  \n",
      "996                                   CPC+ All  CPC Plus Connect  \n",
      "997    AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "998    AG014: Integrating Behavioral Health:CM  CPC Plus Connect  \n",
      "999                     AG010: Team-Based Care  CPC Plus Connect  \n",
      "\n",
      "[1391 rows x 9 columns] is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Dec/2017 10:10:56] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "##Run the app in broswer instead\n",
    "\n",
    "app=dash.Dash()\n",
    "\n",
    "report=pd.read_csv('connect_report.csv')\n",
    "\n",
    "report['Created Date']=pd.to_datetime(report['Created Date'])\n",
    "report=report[pd.notnull(report['Created Date'])]\n",
    "report=report[pd.notnull(report['Body'])]\n",
    "\n",
    "#Removing HTML tags and codes\n",
    "report['Body'] = report['Body'].apply(lambda x:BeautifulSoup(x))\n",
    "report['Body'] = report['Body'].apply(lambda x:x.get_text())\n",
    "\n",
    "#Removing tags/mentions\n",
    "report['Body'] = report['Body'].apply(lambda x:re.sub('{@[\\w\\d]*}', '',x))\n",
    "\n",
    "#Removing urls\n",
    "report['Body'] = report['Body'].apply(lambda x:re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x,flags=re.MULTILINE))\n",
    "\n",
    "#Removing weird unicode characters\n",
    "report['Body'] = report['Body'].apply(lambda x: x.encode('ascii', 'ignore'))\n",
    "\n",
    "final_report=pd.DataFrame(data=report)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colors={\n",
    "    'background': '#111111',\n",
    "    'text': '#7FDBFF'\n",
    "}\n",
    "\n",
    "app.layout=html.Div(children=\n",
    "    [html.H1(children='This is the title pane',\n",
    "             style={\n",
    "                 'textAlign': 'center', 'color':colors['text']}   ),\n",
    "    dcc.Markdown(children=example_text), \n",
    "    html.H2(children='Please use these inputs to customize your report',\n",
    "            style={\n",
    "                 'textAlign': 'center', 'color':colors['text']} ),\n",
    "    dcc.DatePickerRange(\n",
    "        id='date_selector',\n",
    "        min_date_allowed=dt(2017,1,1),\n",
    "        start_date=dt(2017,1,1),\n",
    "        end_date=dt.today()),\n",
    "    html.Table(id='table'), \n",
    "#Need to work on spacing here and then figure out how to use them as arguments for all of the functions  \n",
    "#     html.Label('First word finder'),\n",
    "#     dcc.Input(\n",
    "#         id='first_word_finder',\n",
    "#         value='',\n",
    "#         type='text'),\n",
    "#     html.Label('Second word finder'),\n",
    "#     dcc.Input(value='If you would like to find the intersection of the first word and another please enter here',type='text'),\n",
    "#     ##Try to build the graph here \n",
    "#     dcc.Graph(\n",
    "#         id='test_graph', \n",
    "        \n",
    "#     ),\n",
    "     \n",
    "    \n",
    "     \n",
    "     html.Div(id='output-container-date-selector',style={'display': 'none'}),\n",
    "     html.Div(id='output-container-top-words',style={'display':'none'}),\n",
    "     html.Div(id=\"output-container-scored\",style={'display':'none'})\n",
    "     \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#Need to move things that are setup/dependent on reactive inputs here into here\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output-container-date-selector','children'),\n",
    "[dash.dependencies.Input('date_selector','start_date'),\n",
    " dash.dependencies.Input('date_selector','end_date')]\n",
    ")\n",
    "##going to need to do this in a bunch of different functions?\n",
    "def update_report(input1,input2):\n",
    "    start=pd.to_datetime(input1,utc=False)\n",
    "    end=pd.to_datetime(input2,utc=False)\n",
    "    if(start==\"2017-01-01\" and end== pd.to_datetime('today')):\n",
    "        final_report_reactive=final_report\n",
    "    else:\n",
    "        final_report_reactive=final_report[(final_report['Created Date'] > start) & (final_report['Created Date']< end)]   \n",
    "        json_save=final_report_reactive.to_json(orient='index')\n",
    "        json_string=(json.dumps(json_save))\n",
    "        return(json_string)\n",
    "\n",
    "#Testing on the table\n",
    "@app.callback(dash.dependencies.Output('table','children'),\n",
    "             [dash.dependencies.Input('output-container-date-selector','children')]\n",
    "             )\n",
    "def update_table(input1):\n",
    "    final_report_updated=pd.read_json(json.loads(input1),orient='index')\n",
    "    reactive_table=html.Table(final_report_updated)\n",
    "    return(reactive_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-01-01 00:00:00')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_report_test=final_report[(final_report['Created Date']>\"2017-04-01\") & (final_report['Created Date'] < \"2017-05-01\")]\n",
    "json_test=final_report_test.to_json()\n",
    "\n",
    "json_string=(json.dumps(json_test))\n",
    "\n",
    "pd.read_json(json.loads(json_string))\n",
    "\n",
    "pd.to_datetime(\"1/1/2017\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
